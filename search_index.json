[["index.html", "Computational Genomics with R Book Club Welcome", " Computational Genomics with R Book Club The R4DS Online Learning Community 2022-11-25 Welcome Welcome to the bookclub! This is a companion for the book Computational Genomics with R by Altuna Akalin (Chapman &amp; Hall, copyright 2021, 9781498781855). This companion is available at r4ds.io/comp_genom. This website is being developed by the R4DS Online Learning Community. Follow along, and join the community to participate. This companion follows the R4DS Online Learning Community Code of Conduct. "],["book-club-meetings.html", "Book club meetings", " Book club meetings Each week, a volunteer will present a chapter from the book (or part of a chapter). This is the best way to learn the material. Presentations will usually consist of a review of the material, a discussion, and/or a demonstration of the principles presented in that chapter. More information about how to present is available in the github repo. Presentations will be recorded, and will be available on the R4DS Online Learning Community YouTube Channel. "],["pace.html", "Pace", " Pace We’ll try to cover 1 chapter/week, but… …It’s ok to split chapters when they feel like too much. We will try to meet every week, but will likely take some breaks for holidays, etc. "],["introduction-to-genomics.html", "Chapter 1 Introduction to Genomics", " Chapter 1 Introduction to Genomics Chapter Objectives: Describe what makes up a gene and genome Understand how genes and genomes are regulated Differentiate between the different types of genome mutations List the general steps required of high-throughput sequencing "],["genes-genomes-and-genomics.html", "1.1 Genes, Genomes, and Genomics", " 1.1 Genes, Genomes, and Genomics DNA: the building blocks of life. Gene: A unit of nucleotides that when transcribed, create a functional RNA sequence. RNA: intermediate messenger; different types! Protein: macromolecules consisting of amino acids that are translated from an RNA sequence. Genome: the sum of all genes; the instructions of life. Genomics: The study of genomes (structure, function, sequences) "],["gene-regulation.html", "1.2 Gene regulation", " 1.2 Gene regulation The process by which genes are controlled. When they are turned on or off, how much to make. Often has a cascading effect. 1.2.1 Transcriptional regulation Control output of transcripts before transcription occurs. Transcription factors Epigenetic (histone) modifications 1.2.2 Post-transcriptional regulation Control what happens to transcripts after they are made. Splicing non-coding RNAs (ncRNAs) "],["genetic-mutations.html", "1.3 Genetic mutations", " 1.3 Genetic mutations Gene-level mutations Chromosomal-level mutations "],["sequencing.html", "1.4 Sequencing", " 1.4 Sequencing Types of analyses possible from sequencing data. Source: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4266812/figure/f0025/ "],["resources.html", "1.5 Resources", " 1.5 Resources General knowledge Kahn Academy Genome.gov NGS Sequencing Sequencing by Synthesis (Illumina) Intro to Illumina Seq Illumina Official Video Nanopore (Oxford Nanopore Technologies) Nanopore DNA Sequencing ONT Official Video "],["meeting-videos.html", "1.6 Meeting Videos", " 1.6 Meeting Videos 1.6.1 Cohort 1 Meeting chat log LOG "],["introduction-to-r-for-genomic-data-analysis.html", "Chapter 2 Introduction to R for Genomic Data Analysis", " Chapter 2 Introduction to R for Genomic Data Analysis Learning objectives: Understand the general workflow of genomic analysis in R Comprehend differences between data structures and data types Download and install R, RStudio, Bioconductor, compGenomRData Reading and writing tables Construct plots in base R and/or ggplot Functions and loops "],["genomic-data-analysis-a-high-level-view-in-r.html", "2.1 Genomic data analysis (a high level view)– in R!", " 2.1 Genomic data analysis (a high level view)– in R! A high level overview of genomic analysis. Source: https://www.intechopen.com/chapters/50574 Analyses that can be performed in R. Source: https://image.slideserve.com/835537/bioconductor-l.jpg "],["getting-started-with-r-and-bioconductor.html", "2.2 Getting started with R and Bioconductor", " 2.2 Getting started with R and Bioconductor Install R and RStudio if you haven’t done so already (see resources) Install Bioconductor install.packages(&quot;BiocManager&quot;) https://www.bioconductor.org/install/ Installing Bioconductor packages BiocManager::install(&quot;xyzpackage&quot;) Installing packages from Github library(devtools) devtools::install_github(&quot;compgenomr/compGenomRData&quot;) "],["data-stuctures-and-data-types.html", "2.3 Data stuctures and data types", " 2.3 Data stuctures and data types Data Structures Data types "],["reading-and-writing-data.html", "2.4 Reading and writing data", " 2.4 Reading and writing data First download the compGenomRData package from Github if you haven’t done so yet. We are using this as a companion for the course! devtools::install_github(&quot;compgenomr/compGenomRData&quot;) Read files in base R cpgiFilePath=system.file(&quot;extdata&quot;, &quot;subset.cpgi.hg18.bed&quot;, package=&quot;compGenomRData&quot;) # read CpG island BED file cpgi.df &lt;- read.table(cpgiFilePath, header = FALSE) enhancerFilePath=system.file(&quot;extdata&quot;, &quot;subset.enhancers.hg18.bed&quot;, package=&quot;compGenomRData&quot;) # read enhancer marker BED file enh.df &lt;- read.table(enhancerFilePath, header = FALSE) # check first lines to see how the data looks like head(enh.df) ## V1 V2 V3 V4 V5 V6 V7 V8 V9 ## 1 chr20 266275 267925 . 1000 . 9.11 13.1693 -1 ## 2 chr20 287400 294500 . 1000 . 10.53 13.0231 -1 ## 3 chr20 300500 302500 . 1000 . 9.10 13.3935 -1 ## 4 chr20 330400 331800 . 1000 . 6.39 13.5105 -1 ## 5 chr20 341425 343400 . 1000 . 6.20 12.9852 -1 ## 6 chr20 437975 439900 . 1000 . 6.31 13.5184 -1 the key here is read.table() Reading large tables with readr. library(readr) df.f2=read_table(enhancerFilePath, col_names = FALSE) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## X1 = col_character(), ## X2 = col_double(), ## X3 = col_double(), ## X4 = col_character(), ## X5 = col_double(), ## X6 = col_character(), ## X7 = col_double(), ## X8 = col_double(), ## X9 = col_double() ## ) df.f2 ## # A tibble: 50,416 × 9 ## X1 X2 X3 X4 X5 X6 X7 X8 X9 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 chr20 266275 267925 . 1000 . 9.11 13.2 -1 ## 2 chr20 287400 294500 . 1000 . 10.5 13.0 -1 ## 3 chr20 300500 302500 . 1000 . 9.1 13.4 -1 ## 4 chr20 330400 331800 . 1000 . 6.39 13.5 -1 ## 5 chr20 341425 343400 . 1000 . 6.2 13.0 -1 ## 6 chr20 437975 439900 . 1000 . 6.31 13.5 -1 ## 7 chr20 516650 518525 . 1000 . 12.5 13.7 -1 ## 8 chr20 519100 521475 . 1000 . 7.1 13.1 -1 ## 9 chr20 543800 545775 . 1000 . 9.52 13.0 -1 ## 10 chr20 573550 574975 . 1000 . 7.71 13.7 -1 ## # … with 50,406 more rows Readr is part of the tidyverse (tidyverse is life). Writing data w/ write.table write.table(enh.df,file=&quot;enh.txt&quot;,quote=FALSE, row.names=FALSE,col.names=FALSE,sep=&quot;\\t&quot;) The type of file it is save as depends on the sep (^ saved a tab separated file or .tsv) Saving/loading R objects directly into/from a file # save() saves many objects at once, regardless of class save(cpgi.df,enh.df,file=&quot;mydata.RData&quot;) load(&quot;mydata.RData&quot;) # saveRDS() can save one object at a type saveRDS(cpgi.df,file=&quot;cpgi.rds&quot;) x=readRDS(&quot;cpgi.rds&quot;) head(x) # when using saveRDS() must assign output of readRDS() to new variable in session "],["plots-base-r-vs-ggplot.html", "2.5 Plots: Base R vs ggplot", " 2.5 Plots: Base R vs ggplot A great comparison of the two systems! ggplot: “The basic idea is that you can split a chart into graphical objects — data, scale, coordinate system, and annotation — and think about them separately. When you put it all together, you get a complete chart.” base: “specify everything in the function arguments” "],["functions-and-loops.html", "2.6 Functions and Loops", " 2.6 Functions and Loops Components of a basic function my_function &lt;-function(x,y){ # create function as object (my_function &lt;- function()) # define inputs of function (function(x,y)) # function is written between {} brackets result=x^2+y^2 # what your function does with inputs return(result) # return() gives the result as the output } my_function(2,3) # now try the function out ## [1] 13 A function with conditions largeCpGi&lt;-function(bedRow){ # function takes input one row of CpGi data frame # generated earlier under &quot;Read files in base R&quot; cpglen=bedRow[3]-bedRow[2]+1 # subtract column 3 value from column 2 value + 1 if(cpglen&gt;1500){ # if the result (cpglen) meets this parameter # (is larger than 1500) cat(&quot;this is large\\n&quot;) # cat[used in place of print/return] &quot;this is # large\\n&quot; --&gt; the \\n means print a new line } else if(cpglen&lt;=1500 &amp; cpglen&gt;700){ # else if -- if the above was not true, # see if result meets these parameters cat(&quot;this is normal\\n&quot;) } else{ # if the result did not meet previous parameters cat(&quot;this is short\\n&quot;) } } largeCpGi(cpgi.df[10,]) # function to take 10th row of data frame ## this is short largeCpGi(cpgi.df[10]) # what happens if you input this instead? ## Error in `[.data.frame`(cpgi.df, 10): undefined columns selected largeCpGi(cpgi.df(10)) # or this? ## Error in cpgi.df(10): could not find function &quot;cpgi.df&quot; Loops to repeat a function multiple times result=c() # this is where we will keep the lengths. for now it is an empty vector. for(i in 1:100){ # start a loop to iterate 100 times len=cpgi.df[i,3]-cpgi.df[i,2]+1 # to calculate the length of each cpgi # where i is each row of the data frame cpgi.df result=c(result,len) # append the length to the result each time } result # check the result ## [1] 277 360 383 1325 3498 1602 1317 250 871 226 788 300 1001 311 790 ## [16] 224 1259 725 525 690 426 636 827 1010 237 601 484 297 968 441 ## [31] 793 235 212 648 656 659 794 324 356 729 786 202 401 1787 1045 ## [46] 265 203 498 455 203 821 745 1013 341 1795 801 333 885 520 497 ## [61] 1403 1737 1192 1007 1142 1009 276 626 1440 512 552 344 268 233 273 ## [76] 232 2353 456 937 941 985 1584 878 357 758 884 1355 979 1294 757 ## [91] 786 807 1118 833 931 533 618 1965 321 1048 Loops are not always the most efficient way to work in R. We can use special functions to perform loops. In base R: the apply family lapply applies a function to a single list or vector mapply is like lapply but over multiple vectors/lists vectorized function can be summed (+, rowSums, colSums) In the tidyverse: purrr –&gt; specifically map "],["resources-1.html", "2.7 Resources", " 2.7 Resources R and R Studio R R Studio Bioconductor Tidyverse apply functions purr and map "],["meeting-videos-1.html", "2.8 Meeting Videos", " 2.8 Meeting Videos 2.8.1 Cohort 1 Meeting chat log LOG "],["statistics-for-genomics.html", "Chapter 3 Statistics for Genomics", " Chapter 3 Statistics for Genomics Learning objectives: understand your data apply statistical methods for analyzing it test differences "],["introduction.html", "3.1 Introduction", " 3.1 Introduction In this chapter we will see some statistical techniques that are used for analyzing data, in particular for genomics data. We will have a look at the descriptive statistics and more common distributions. Finally, we will attempt at answering some questions, and in order to do that, we will use regression re-sampling methods. To conclude, we will discuss about the possible approaches for best estimation and make some metrics both for regression and classification models. "],["how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions.html", "3.2 How to summarize collection of data points: The idea behind statistical distributions", " 3.2 How to summarize collection of data points: The idea behind statistical distributions 3.2.1 Measuring the measuring central tendency mean median: not affected by outliers 3.2.2 Measurements of variation range standard deviation variance: affected by outliers adj variance percentiles: difference between 75th percentile and 25th percentile removes potential outliers 3.2.3 Statistical distributions probability of occurrence normal distribution or Gaussian distribution: typical “bell-curve” 3.2.4 Confidence intervals bootstrap resampling or bootstrapping: estimate intervals is to repeatedly take samples from the original sample with replacement. library(mosaic) set.seed(21) sample1= rnorm(50,20,5) # simulate a sample # do bootstrap resampling, sampling with replacement boot.means=do(1000) * mean(resample(sample1)) # get percentiles from the bootstrap means q=quantile(boot.means[,1],p=c(0.025,0.975)) # plot the histogram hist(boot.means[,1],col=&quot;cornflowerblue&quot;,border=&quot;white&quot;, xlab=&quot;sample means&quot;) abline(v=c(q[1], q[2] ),col=&quot;red&quot;) text(x=q[1],y=200,round(q[1],3),adj=c(1,0)) text(x=q[2],y=200,round(q[2],3),adj=c(0,0)) Central Limit Theorem(CLT): construct the confidence interval using standard normal distribution, take repeated samples from a population with sample size, the distribution of means of those samples will be approximately normal with mean and standard deviation. "],["how-to-test-for-differences-between-samples.html", "3.3 How to test for differences between samples", " 3.3 How to test for differences between samples Variability in our measurements healthy samples are different from disease samples in some measurable feature (blood count, gene expression, methylation of certain loci). subtract the means of two samples hypothesis testing “we can compare the real difference and measure how unlikely it is to get such a value under the expectation of the null hypothesis” 3.3.1 Randomization set.seed(100) gene1=rnorm(30,mean=4,sd=2) gene2=rnorm(30,mean=2,sd=2) org.diff=mean(gene1)-mean(gene2) gene.df=data.frame(exp=c(gene1,gene2), group=c( rep(&quot;test&quot;,30),rep(&quot;control&quot;,30) ) ) exp.null &lt;- do(1000) * diff(mosaic::mean(exp ~ shuffle(group), data=gene.df)) hist(exp.null[,1],xlab=&quot;null distribution | no difference in samples&quot;, main=expression(paste(H[0],&quot; :no difference in means&quot;) ), xlim=c(-2,2),col=&quot;cornflowerblue&quot;,border=&quot;white&quot;) abline(v=quantile(exp.null[,1],0.95),col=&quot;red&quot; ) abline(v=org.diff,col=&quot;blue&quot; ) text(x=quantile(exp.null[,1],0.95),y=200,&quot;0.05&quot;,adj=c(1,0),col=&quot;red&quot;) text(x=org.diff,y=200,&quot;org. diff.&quot;,adj=c(1,0),col=&quot;blue&quot;) p.val=sum(exp.null[,1]&gt;org.diff)/length(exp.null[,1]) p.val ## [1] 0.001 3.3.2 t-test # Welch&#39;s t-test stats::t.test(gene1,gene2) ## ## Welch Two Sample t-test ## ## data: gene1 and gene2 ## t = 3.7653, df = 47.552, p-value = 0.0004575 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.872397 2.872761 ## sample estimates: ## mean of x mean of y ## 4.057728 2.185149 # t-test with equal variance assumption stats::t.test(gene1,gene2,var.equal=TRUE) ## ## Two Sample t-test ## ## data: gene1 and gene2 ## t = 3.7653, df = 58, p-value = 0.0003905 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.8770753 2.8680832 ## sample estimates: ## mean of x mean of y ## 4.057728 2.185149 3.3.3 Multiple testing # BiocManager::install(&quot;qvalue&quot;) library(qvalue) data(hedenfalk) qvalues &lt;- qvalue(hedenfalk$p)$q bonf.pval=p.adjust(hedenfalk$p,method =&quot;bonferroni&quot;) fdr.adj.pval=p.adjust(hedenfalk$p,method =&quot;fdr&quot;) plot(hedenfalk$p,qvalues,pch=19,ylim=c(0,1), xlab=&quot;raw P-values&quot;,ylab=&quot;adjusted P-values&quot;) points(hedenfalk$p,bonf.pval,pch=19,col=&quot;red&quot;) points(hedenfalk$p,fdr.adj.pval,pch=19,col=&quot;blue&quot;) legend(&quot;bottomright&quot;,legend=c(&quot;q-value&quot;,&quot;FDR (BH)&quot;,&quot;Bonferroni&quot;), fill=c(&quot;black&quot;,&quot;blue&quot;,&quot;red&quot;)) 3.3.4 Moderated t-tests For example, if you have many variances calculated for thousands of genes across samples, you can force individual variance estimates to shrink toward the mean or the median of the distribution of variances. How much the values are shrunk toward a common value depends on the exact method used. These tests in general are called moderated t-tests or shrinkage t-tests. One approach popularized by Limma software is to use so-called “Empirical Bayes methods”. The main formulation in these methods is \\[\\hat{V_g}=aV_0+bV_g\\] \\[V_0=\\text{background variability (the prior)}\\] \\[V_g=\\text{is the individual variability}\\] \\[\\hat{V_g}=\\text{“shrunk” version of the variability.}\\] is the background variability and is the individual variability. Then, these methods estimate and in various ways to come up with a “shrunk” version of the variability. Bayesian inference can make use of prior knowledge to make inference about properties of the data. In a Bayesian viewpoint, the prior knowledge, in this case variability of other genes, can be used to calculate the variability of an individual gene. In our case, would be the prior knowledge we have on the variability of the genes and we use that knowledge to influence our estimate for the individual genes. set.seed(100) #sample data matrix from normal distribution gset=rnorm(3000,mean=200,sd=70) data=matrix(gset,ncol=6) # set groups group1=1:3 group2=4:6 n1=3 n2=3 dx=rowMeans(data[,group1])-rowMeans(data[,group2]) require(matrixStats) # get the estimate of pooled variance stderr = sqrt( (rowVars(data[,group1])*(n1-1) + rowVars(data[,group2])*(n2-1)) / (n1+n2-2) * ( 1/n1 + 1/n2 )) # do the shrinking towards median mod.stderr = (stderr + median(stderr)) / 2 # moderation in variation # estimate t statistic with moderated variance t.mod &lt;- dx / mod.stderr # calculate P-value of rejecting null p.mod = 2*pt( -abs(t.mod), n1+n2-2 ) # estimate t statistic without moderated variance t = dx / stderr # calculate P-value of rejecting null p = 2*pt( -abs(t), n1+n2-2 ) par(mfrow=c(1,2)) hist(p,col=&quot;cornflowerblue&quot;,border=&quot;white&quot;,main=&quot;&quot;,xlab=&quot;P-values t-test&quot;) mtext(paste(&quot;signifcant tests:&quot;,sum(p&lt;0.05)) ) hist(p.mod,col=&quot;cornflowerblue&quot;,border=&quot;white&quot;,main=&quot;&quot;, xlab=&quot;P-values mod. t-test&quot;) mtext(paste(&quot;signifcant tests:&quot;,sum(p.mod&lt;0.05)) ) "],["relationship-between-variables-linear-models-and-correlation.html", "3.4 Relationship between variables: Linear models and correlation", " 3.4 Relationship between variables: Linear models and correlation Examples: want to know about expression of a particular gene in liver in relation to the dosage of a drug that patient receives DNA methylation of a certain locus in the genome in relation to the age of the sample donor. relationship between histone modifications and gene expression. \\[Y=\\beta_0+\\beta_1X+\\epsilon\\] Approximation of the response: \\[Y\\sim\\beta_0+\\beta_1X\\] Estimation of the coefficients: \\[Y=\\hat{\\beta_0}+\\hat{\\beta_1}X\\] More than one predictor: \\[Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+\\epsilon\\] \\[Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+\\beta_3X_3+...+\\beta_nX_n+\\epsilon\\] \\[Y=\\begin{bmatrix} 1 &amp; X_{1,1} &amp; X_{1,2}\\\\ 1 &amp; X_{2,1} &amp; X_{2,2}\\\\ 1 &amp; X_{3,1} &amp; X_{3,2}\\\\ 1 &amp; X_{4,1} &amp; X_{4,2} \\end{bmatrix}\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\end{bmatrix}+\\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3\\\\ \\epsilon_0 \\end{bmatrix}\\] \\[Y_1=\\beta_0+\\beta_1X_{1,1}+\\beta_2X_{1,2}+\\epsilon_1\\] \\[Y_1=\\beta_0+\\beta_1X_{2,1}+\\beta_2X_{2,2}+\\epsilon_2\\] \\[Y_1=\\beta_0+\\beta_1X_{3,1}+\\beta_2X_{3,2}+\\epsilon_3\\] \\[Y_1=\\beta_0+\\beta_1X_{4,1}+\\beta_2X_{4,2}+\\epsilon_4\\] \\[Y=X\\beta+\\epsilon\\] 3.4.1 The cost or loss function approach Minimize the residuals: optimization procedure \\[min\\sum{(y_i=(\\beta_0+\\beta_1x_1))^2}\\] The “gradient descent” algorithm” Pick a random starting point, random \\(\\beta\\) values. Take the partial derivatives of the cost function to see which direction is the way to go in the cost function. Take a step toward the direction that minimizes the cost function. Step size is a parameter to choose, there are many variants. Repeat step 2,3 until convergence. The algorithm usually converges to optimum \\(\\beta\\) values. Figure 3.1: Cost function landscape for linear regression with changing beta values. The optimization process tries to find the lowest point in this landscape by implementing a strategy for updating beta values toward the lowest point in the landscape. 3.4.2 The “maximum likelihood” approach The response variable \\(y_i\\) follows a normal distribution with mean \\(\\beta_0+\\beta_1x_i\\) and variance \\(s^2\\). Find \\(\\beta_0\\) and \\(\\beta_1\\) that maximizes the probability of observing all the response variables in the dataset given the explanatory variables. constant variance \\(s^2\\), estimation of the variance of the population \\(\\sigma^2\\) \\[s^2=\\frac{\\sum{\\epsilon_i}}{n-2}\\] Probability: \\[P(y_i)=\\frac{1}{s\\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{y_i-(\\beta_0+\\beta_1x_i)}{s})^2}\\] Likelihood function: \\[L=P(y_1)P(y_2)P(y_3)..P(y_n)=\\prod_{i=1}^{n}{P_i}\\] The log: \\[ln(L)=-nln(s\\sqrt{2\\pi})-\\frac{1}{2s^2}\\sum_{i=1}{n}{(y_i-(\\beta_0+\\beta_1x_i))^2}\\] the negative of the cost function: \\[-\\frac{1}{2s^2}\\sum_{i=1}{n}{(y_i-(\\beta_0+\\beta_1x_i))^2}\\] to optimize this function we would need to take the derivative of the function with respect to the parameters. 3.4.3 Linear algebra and closed-form solution to linear regression \\[\\epsilon_i=Y_i-(\\beta_0+\\beta_1x_i)\\] \\[\\sum{\\epsilon_i^2}=\\epsilon^T\\epsilon=(Y-\\beta X)^T(Y-\\beta X)\\] \\[= Y^TY-(2\\beta^T Y + \\beta^T X^T X\\beta)\\] \\[\\hat{\\beta}=(X^T X)^{-1}X^T Y\\] \\[\\hat{\\beta_1}=\\frac{\\sum{(x_i-\\bar{X})(y_i-\\bar{Y})}}{\\sum{(x_i-\\bar{X})^2}}\\] \\[\\hat{\\beta_0}=\\bar{Y}-\\hat{\\beta_1}\\bar{X}\\] # set random number seed, so that the random numbers from the text # is the same when you run the code. set.seed(32) # get 50 X values between 1 and 100 x = runif(50,1,100) # set b0,b1 and variance (sigma) b0 = 10 b1 = 2 sigma = 20 # simulate error terms from normal distribution eps = rnorm(50,0,sigma) # get y values from the linear equation and addition of error terms y = b0 + b1*x+ eps mod1=lm(y~x) # plot the data points plot(x,y,pch=20, ylab=&quot;Gene Expression&quot;,xlab=&quot;Histone modification score&quot;) # plot the linear fit abline(mod1,col=&quot;blue&quot;) 3.4.4 How to estimate the error of the coefficients Figure 3.2: Regression coefficients vary with every random sample. The figure illustrates the variability of regression coefficients when regression is done using a sample of data points. Histograms depict this variability for b0 and b1 coefficients. The calculation of the Residual Standard Error (RSE) \\[s=RSE\\] mod1=lm(y~x) summary(mod1) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -38.50 -12.62 1.66 14.26 37.20 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.40951 6.43208 0.064 0.95 ## x 2.08742 0.09775 21.355 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 16.96 on 48 degrees of freedom ## Multiple R-squared: 0.9048, Adjusted R-squared: 0.9028 ## F-statistic: 456 on 1 and 48 DF, p-value: &lt; 2.2e-16 # get confidence intervals confint(mod1) ## 2.5 % 97.5 % ## (Intercept) -12.523065 13.342076 ## x 1.890882 2.283952 # pull out coefficients from the model coef(mod1) ## (Intercept) x ## 0.4095054 2.0874167 3.4.5 Accuracy of the model \\[s=RSE=\\sqrt{\\frac{\\sum{(y_i-\\hat{Y_i})^2}}{n-p}}=\\sqrt{\\frac{RSS}{n-p}}\\] \\[R^2=1-\\frac{RSS}{TSS}=\\frac{TSS-RSS}{TSS}=1-\\frac{RSS}{TSS}\\] \\[r_{xy}=\\frac{cov(X,Y)}{\\sigma_x\\sigma_y}=\\frac{\\sum_{i=1}^n{(x_i-\\bar{x})(y_i-\\bar{y})}}{\\sqrt{\\sum_{i=1}^n{(x_i-\\bar{x})^2\\sum_{i=1}^n{(y_i-\\bar{y})^2}}}}\\] Figure 3.3: Correlation and covariance for different scatter plots. \\[H_0:\\beta_1=\\beta_2=\\beta_3=...=\\beta_p=0\\] \\[H_1:\\text{at least one}\\beta_1\\neq0\\] The F-statistic for a linear model \\[F=\\frac{(TSS-RSS)/(p-1)}{RSS/(n-p)}=\\frac{(TSS-RSS)/(p-1)}{RSE}\\sim{F(p-1,n-p)}\\] 3.4.6 Regression with categorical variables set.seed(100) gene1=rnorm(30,mean=4,sd=2) gene2=rnorm(30,mean=2,sd=2) gene.df=data.frame(exp=c(gene1,gene2), group=c( rep(1,30),rep(0,30) ) ) mod2=lm(exp~group,data=gene.df) summary(mod2) ## ## Call: ## lm(formula = exp ~ group, data = gene.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.7290 -1.0664 0.0122 1.3840 4.5629 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.1851 0.3517 6.214 6.04e-08 *** ## group 1.8726 0.4973 3.765 0.000391 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.926 on 58 degrees of freedom ## Multiple R-squared: 0.1964, Adjusted R-squared: 0.1826 ## F-statistic: 14.18 on 1 and 58 DF, p-value: 0.0003905 require(mosaic) plotModel(mod2) gene.df=data.frame(exp=c(gene1,gene2,gene2), group=c( rep(&quot;A&quot;,30),rep(&quot;B&quot;,30),rep(&quot;C&quot;,30) ) ) mod3=lm(exp~group,data=gene.df) summary(mod3) ## ## Call: ## lm(formula = exp ~ group, data = gene.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.7290 -1.0793 -0.0976 1.4844 4.5629 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.0577 0.3781 10.731 &lt; 2e-16 *** ## groupB -1.8726 0.5348 -3.502 0.000732 *** ## groupC -1.8726 0.5348 -3.502 0.000732 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.071 on 87 degrees of freedom ## Multiple R-squared: 0.1582, Adjusted R-squared: 0.1388 ## F-statistic: 8.174 on 2 and 87 DF, p-value: 0.0005582 3.4.7 Regression pitfalls Non-linearity Correlation of explanatory variables Correlation of error terms Non-constant variance of error terms Outliers and high leverage points "],["meeting-videos-2.html", "3.5 Meeting Videos", " 3.5 Meeting Videos 3.5.1 Cohort 1 Meeting chat log LOG "],["exploratory-data-analysis-with-unsupervised-machine-learning.html", "Chapter 4 Exploratory Data Analysis with Unsupervised Machine Learning ", " Chapter 4 Exploratory Data Analysis with Unsupervised Machine Learning "],["very-helpful-youtube-explanations.html", "4.1 Very helpful youtube explanations", " 4.1 Very helpful youtube explanations I think I’ve mentioned StatQuest at almost every bookclub meeting, but these are the clearest and most intuitive explanations I’ve found regarding biostatistics. PCA concepts PCA step-by-step Even more PCA tips MDS tSNE Not a StatQuest video, but useful in explaining NNMF in the context of cancer mutation signatures Recommend skipping to 16:45. "],["chapter-4-exercises.html", "4.2 Chapter 4 Exercises", " 4.2 Chapter 4 Exercises Data set and packages expFile=system.file(&quot;extdata&quot;, &quot;leukemiaExpressionSubset.rds&quot;, package=&quot;compGenomRData&quot;) mat=readRDS(expFile) library(pheatmap) library(cluster) library(fastICA) library(Rtsne) ## 1 x &lt;- scale(mat) y &lt;- scale(log2(mat)) boxplot(mat) boxplot(x) boxplot(y) ## 2 annotation_col = data.frame(LeukemiaType =substr(colnames(mat),1,3)) rownames(annotation_col)=colnames(mat) pheatmap(mat,show_rownames=FALSE,show_colnames=FALSE, annotation_col=annotation_col, scale = &quot;none&quot;,clustering_method=&quot;ward.D2&quot;, clustering_distance_cols=&quot;euclidean&quot;) pheatmap(mat,show_rownames=FALSE,show_colnames=FALSE, annotation_col=annotation_col, scale = &quot;none&quot;,clustering_method=&quot;ward.D&quot;, clustering_distance_cols=&quot;euclidean&quot;) ### IMO this one looks the bet to me pheatmap(x,show_rownames=FALSE,show_colnames=FALSE, annotation_col=annotation_col, scale = &quot;none&quot;,clustering_method=&quot;ward.D&quot;, clustering_distance_cols=&quot;euclidean&quot;) pheatmap(y,show_rownames=FALSE,show_colnames=FALSE, annotation_col=annotation_col, scale = &quot;none&quot;,clustering_method=&quot;ward.D&quot;, clustering_distance_cols=&quot;euclidean&quot;) pheatmap(mat,show_rownames=FALSE,show_colnames=FALSE, annotation_col=annotation_col, scale = &quot;column&quot;,clustering_method=&quot;ward.D&quot;, clustering_distance_cols=&quot;euclidean&quot;) ## 3 number of clusters set.seed(101) pamclu=cluster::pam(t(mat),k=5) plot(silhouette(pamclu),main=NULL) # even when mat is changed with scaled data it looks the same Ks=sapply(2:7, function(i) summary(silhouette(pam(t(mat),k=i)))$avg.width) plot(2:7,Ks,xlab=&quot;k&quot;,ylab=&quot;av. silhouette&quot;,type=&quot;b&quot;, pch=19) ## 4 set.seed(101) # define the clustering function pam1 &lt;- function(x,k) list(cluster = pam(x,k, cluster.only=TRUE)) #calculate the gap statistic pam.gap= clusGap(t(mat), FUN = pam1, K.max = 8,B=50) #plot the gap statistic accross k values plot(pam.gap, main = &quot;Gap statistic for the &#39;Leukemia&#39; data&quot;) ## 5 #data all together doesn&#39;t tell you anything useful plot(mat, pch = 19, col=as.factor(annotation_col$LeukemiaType)) princomp(x) ## Call: ## princomp(x = x) ## ## Standard deviations: ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 ## 5.3980464 3.1529029 2.3247277 1.5687082 1.1441699 1.0163041 0.9208557 0.8361879 ## Comp.9 Comp.10 Comp.11 Comp.12 Comp.13 Comp.14 Comp.15 Comp.16 ## 0.8063709 0.7415128 0.7349786 0.6843525 0.6582846 0.6364035 0.6263027 0.6153725 ## Comp.17 Comp.18 Comp.19 Comp.20 Comp.21 Comp.22 Comp.23 Comp.24 ## 0.6003703 0.5690649 0.5493224 0.5247540 0.5098144 0.4898736 0.4824333 0.4733643 ## Comp.25 Comp.26 Comp.27 Comp.28 Comp.29 Comp.30 Comp.31 Comp.32 ## 0.4628049 0.4496037 0.4392058 0.4247633 0.4218220 0.3913511 0.3829944 0.3730724 ## Comp.33 Comp.34 Comp.35 Comp.36 Comp.37 Comp.38 Comp.39 Comp.40 ## 0.3649651 0.3553183 0.3382413 0.3265285 0.3228467 0.3163246 0.3085226 0.2966597 ## Comp.41 Comp.42 Comp.43 Comp.44 Comp.45 Comp.46 Comp.47 Comp.48 ## 0.2874149 0.2799747 0.2500159 0.2453183 0.2348965 0.2282003 0.2246586 0.2127631 ## Comp.49 Comp.50 Comp.51 Comp.52 Comp.53 Comp.54 Comp.55 Comp.56 ## 0.2052918 0.1851652 0.1791513 0.1739143 0.1657289 0.1652361 0.1570080 0.1543267 ## Comp.57 Comp.58 Comp.59 Comp.60 ## 0.1381601 0.1305611 0.1283559 0.1184983 ## ## 60 variables and 1000 observations. screeplot(princomp(x)) #create the subset of the data with two genes only. notice that we transpose the matrix so samples are on the columns par(mfrow=c(1,2)) sub.mat=t(mat[rownames(mat) %in% c(&quot;ENSG00000100504&quot;,&quot;ENSG00000105383&quot;),]) #create the subset of the data with two genes only. notice that we transpose the matrix so samples are on the columns plot(scale(mat[rownames(mat)==&quot;ENSG00000100504&quot;,]), scale(mat[rownames(mat)==&quot;ENSG00000105383&quot;,]), pch=19, ylab=&quot;CD33 (ENSG00000105383)&quot;, xlab=&quot;PYGL (ENSG00000100504)&quot;, col=as.factor(annotation_col$LeukemiaType), xlim=c(-2,2),ylim=c(-2,2)) #create the legend for the Leukemia types legend(&quot;bottomright&quot;, legend=unique(annotation_col$LeukemiaType), fill =palette(&quot;default&quot;), border=NA,box.col=NA) # calculate the PCA only for our genes and all the samples pr=princomp(scale(sub.mat)) #screeplot of PCA screeplot(pr) #plot the direction of eigenvectors #pr$loadings returned by princomp has the eigenvectors arrows(x0=0, y0=0, x1 = pr$loadings[1,1], y1 = pr$loadings[2,1],col=&quot;pink&quot;,lwd=3) arrows(x0=0, y0=0, x1 = pr$loadings[1,2], y1 = pr$loadings[2,2],col=&quot;gray&quot;,lwd=3) #plot the samples in the new coordinate system plot(-pr$scores,pch=19, col=as.factor(annotation_col$LeukemiaType), ylim=c(-2,2),xlim=c(-4,4)) #plot the new coordinate basis vectors arrows(x0=0, y0=0, x1 =-2, y1 = 0,col=&quot;pink&quot;,lwd=3) arrows(x0=0, y0=0, x1 = 0, y1 = -1,col=&quot;gray&quot;,lwd=3) ## 6 par(mfrow=c(1,2)) d=svd(scale(mat)) # apply SVD assays=t(d$u) %*% scale(mat) # projection on eigenassays plot(assays[1,],assays[2,],pch=19, col=as.factor(annotation_col$LeukemiaType)) pr=prcomp(t(mat),center=TRUE,scale=TRUE) # apply PCA on transposed matrix # plot new coordinates from PCA, projections on eigenvectorssince the matrix is transposed eigenvectors represent plot(pr$x[,1],pr$x[,2],col=as.factor(annotation_col$LeukemiaType)) ## 7 # apply ICA ica.res=fastICA(t(mat),n.comp=2) # plot reduced dimensions plot(ica.res$S[,1],ica.res$S[,2],col=as.factor(annotation_col$LeukemiaType)) ## 8 set.seed(42) # Set a seed if you want reproducible results tsne_out &lt;- Rtsne(t(mat),perplexity = 10) # Run TSNE # image(t(as.matrix(dist(tsne_out$Y)))) # Show the objects in the 2D tsne representation plot(tsne_out$Y,col=as.factor(annotation_col$LeukemiaType),pch=19) legend(&quot;bottomleft&quot;, # create the legend for the Leukemia types legend=unique(annotation_col$LeukemiaType), fill =palette(&quot;default&quot;), border=NA,box.col=NA) "],["meeting-videos-3.html", "4.3 Meeting Videos", " 4.3 Meeting Videos 4.3.1 Cohort 1 Meeting chat log LOG "],["predictive-modeling-with-supervised-machine-learning.html", "Chapter 5 Predictive Modeling with Supervised Machine Learning", " Chapter 5 Predictive Modeling with Supervised Machine Learning Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["and-5.html", "5.1 5.1 and 5.2", " 5.1 5.1 and 5.2 Statistics vs Machine Learning. Source: https://stats.stackexchange.com/questions/442128/machine-learning-vs-statistical-learning-vs-statistics Unsupervised Machine Learning. Source: https://www.analytixlabs.co.in/blog/types-of-machine-learning/ ## 5.3 Example Case (Prep) # get file paths fileLGGexp=system.file(&quot;extdata&quot;, &quot;LGGrnaseq.rds&quot;, package=&quot;compGenomRData&quot;) fileLGGann=system.file(&quot;extdata&quot;, &quot;patient2LGGsubtypes.rds&quot;, package=&quot;compGenomRData&quot;) # gene expression values gexp=readRDS(fileLGGexp) head(gexp[,1:5]) ## TCGA-CS-4941 TCGA-CS-4944 TCGA-CS-5393 TCGA-CS-5394 TCGA-CS-5395 ## A1BG 72.2326 24.7132 46.3789 37.9659 19.5162 ## A1CF 0.0000 0.0000 0.0000 0.0000 0.0000 ## A2BP1 524.4997 105.4092 323.5828 19.7390 299.5375 ## A2LD1 144.0856 18.0154 29.0942 7.5945 202.1231 ## A2ML1 521.3941 159.3746 164.6157 63.5664 953.4106 ## A2M 17944.7205 10894.9590 16480.1130 9217.7919 10801.8461 dim(gexp) ## [1] 20501 184 # patient annotation patient=readRDS(fileLGGann) head(patient) ## subtype ## TCGA-FG-8185 CIMP ## TCGA-DB-5276 CIMP ## TCGA-P5-A77X CIMP ## TCGA-IK-8125 CIMP ## TCGA-DU-A5TR CIMP ## TCGA-E1-5311 CIMP dim(patient) ## [1] 184 1 #caret for pre-processing library(caret) ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:mosaic&#39;: ## ## dotPlot "],["data-pre-processing.html", "5.2 5.4 Data pre-processing", " 5.2 5.4 Data pre-processing Transform –&gt; filter/scale –&gt; handle missing values #some of the un-transformed data boxplot(gexp[,1:50],outline=FALSE,col=&quot;cornflowerblue&quot;) #looks a little funny... let&#39;s see what happens when we log transform it. par(mfrow=c(1,2)) ##not transformed hist(gexp[,5],xlab=&quot;gene expression&quot;,main=&quot;&quot;,border=&quot;blue4&quot;, col=&quot;cornflowerblue&quot;) #log transformed hist(log10(gexp+1)[,5], xlab=&quot;gene expression log scale&quot;,main=&quot;&quot;, border=&quot;blue4&quot;,col=&quot;cornflowerblue&quot;) # transpose data so predictor variables are on column side tgexp &lt;-t(gexp) Transform –&gt; filter/scale –&gt; handle missing values Remove near zero variation for the columns at least b/c likely not to have predictive value library(caret) # 85% of the values are the same # this function creates the filter but doesn&#39;t apply it yet nzv=preProcess(tgexp,method=&quot;nzv&quot;,uniqueCut = 15) # apply the filter using &quot;predict&quot; function # return the filtered dataset and assign it to nzv_tgexp variable nzv_tgexp=predict(nzv,tgexp) How many variable predictors? This can be arbitrary SDs=apply(tgexp,2,sd ) topPreds=order(SDs,decreasing = TRUE)[1:1000] tgexp=tgexp[,topPreds] What about centering and scaling the data? processCenter=preProcess(tgexp, method = c(&quot;center&quot;)) tgexp=predict(processCenter,tgexp) Filter out highly correlated variables so the model is fitted faster (optional depending on the type of analysis) corrFilt=preProcess(tgexp, method = &quot;corr&quot;,cutoff = 0.9) tgexp=predict(corrFilt,tgexp) Transform –&gt; filter/scale –&gt; handle missing values NA is NOT ZERO!! Options for dealing with missing values … it depends. -Discard samples and/or predictors with missing values -Impute missing values via algorithm #add an NA to the dataset missing_tgexp=tgexp missing_tgexp[1,1]=NA #remove from data set gexpnoNA=missing_tgexp[ , colSums(is.na(missing_tgexp)) == 0] #impute missing value w/ caret::preProcess mImpute=preProcess(missing_tgexp,method=&quot;medianImpute&quot;) imputedGexp=predict(mImpute,missing_tgexp) #another, possibly more accurate imputation via nearest neighbors library(RANN) knnImpute=preProcess(missing_tgexp,method=&quot;knnImpute&quot;) knnimputedGexp=predict(knnImpute,missing_tgexp) "],["split-the-data.html", "5.3 5.5 Split the data", " 5.3 5.5 Split the data “Gold standard” is to split 30% of data as test data, the other 70% tgexp=merge(patient,tgexp,by=&quot;row.names&quot;) # push sample ids back to the row names rownames(tgexp)=tgexp[,1] tgexp=tgexp[,-1] set.seed(3031) # set the random number seed for reproducibility # get indices for 70% of the data set intrain &lt;- createDataPartition(y = tgexp[,1], p= 0.7)[[1]] # separate test and training sets training &lt;- tgexp[intrain,] testing &lt;- tgexp[-intrain,] "],["some-unsupervised-learning-on-training-data-to-initially-cluster-data.html", "5.4 5.6 Some unsupervised learning on training data to initially cluster data", " 5.4 5.6 Some unsupervised learning on training data to initially cluster data Any way you want… k-nearest neighbors is often a good choice. It’s up to you to determine how your data best make sense to group together. library(caret) knnFit=knn3(x=training[,-1], # training set y=training[,1], # training set class labels k=5) # predictions on the test set (which is also the training set in this case) trainPred=predict(knnFit,training[,-1]) "],["section.html", "5.5 5.7", " 5.5 5.7 "],["section-1.html", "5.6 5.8", " 5.6 5.8 "],["meeting-videos-4.html", "5.7 Meeting Videos", " 5.7 Meeting Videos 5.7.1 Cohort 1 Meeting chat log LOG "],["operations-on-genomic-intervals-and-genome-arithmetic.html", "Chapter 6 Operations on Genomic Intervals and Genome Arithmetic", " Chapter 6 Operations on Genomic Intervals and Genome Arithmetic Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1.html", "6.1 SLIDE 1", " 6.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-5.html", "6.2 Meeting Videos", " 6.2 Meeting Videos 6.2.1 Cohort 1 Meeting chat log LOG "],["quality-check-processing-and-alignment-of-high-throughput-sequencing-reads.html", "Chapter 7 Quality Check, Processing and Alignment of High-throughput Sequencing Reads", " Chapter 7 Quality Check, Processing and Alignment of High-throughput Sequencing Reads Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-1.html", "7.1 SLIDE 1", " 7.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-6.html", "7.2 Meeting Videos", " 7.2 Meeting Videos 7.2.1 Cohort 1 Meeting chat log LOG "],["rna-seq-analysis.html", "Chapter 8 RNA-seq Analysis", " Chapter 8 RNA-seq Analysis Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-2.html", "8.1 SLIDE 1", " 8.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-7.html", "8.2 Meeting Videos", " 8.2 Meeting Videos 8.2.1 Cohort 1 Meeting chat log LOG "],["chip-seq-analysis.html", "Chapter 9 ChIP-seq analysis", " Chapter 9 ChIP-seq analysis Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-3.html", "9.1 SLIDE 1", " 9.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-8.html", "9.2 Meeting Videos", " 9.2 Meeting Videos 9.2.1 Cohort 1 Meeting chat log LOG "],["dna-methylation-analysis-using-bisulfite-sequencing-data.html", "Chapter 10 DNA methylation analysis using bisulfite sequencing data", " Chapter 10 DNA methylation analysis using bisulfite sequencing data Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-4.html", "10.1 SLIDE 1", " 10.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-9.html", "10.2 Meeting Videos", " 10.2 Meeting Videos 10.2.1 Cohort 1 Meeting chat log LOG "],["multi-omics-analysis.html", "Chapter 11 Multi-omics Analysis", " Chapter 11 Multi-omics Analysis Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-5.html", "11.1 SLIDE 1", " 11.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-10.html", "11.2 Meeting Videos", " 11.2 Meeting Videos 11.2.1 Cohort 1 Meeting chat log LOG "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
